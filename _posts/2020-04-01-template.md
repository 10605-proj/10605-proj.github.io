---
layout: post
title: Example content (Images and Assets)
tags: [test, tutorial, markdown]
authors: Doe, John, School of Life; Doe, Jane, A School
---
  
  
# Preamble

For efficient training of large-scale deep learning models, parallel/distributed computation should ideally exhibit minimal communication time between workers and good performance on testing data. The state of art for distributed training, mini-batch stochastic gradient descent, uses vectorized implementation for faster and efficient computations but faces speedups and generalization issues especially in the regime of large batch size. The current research interest centers around the question of how to maximize communication efficiency during updates without sacrificing the ability to generalize to test datasets.

## Local SGD: Improving the Computation-communication Trade-off
### Mini-batch SGD 
Mini-batch SGD for K workers:
  
<p align="center">
	<img src="{{ site.url }}/public/images/mini_batch_sgd.png" height="35" />
</p>
  
Mini-batch SGD does not support speedups that scale linearly with the number of machines. With increased communication time between machines and diminishing return of additional gradients, SGD with larger batches often fails to achieve significant reduction in training time. Furthermore, the generalization gap in large-batch training has been widely recognized and studied. [Keskar et al. (2017)](https://arxiv.org/abs/1609.04836) investigated the training and testing accuracy of large-batch and small-batch as a function of epochs. There was a generalization gap between the test accuracy of the two methods, as large batch methods converge to lower test accuracy. Using the concept of [sharp and flat minimizers](https://www.researchgate.net/publication/14100213_Flat_Minima), one explanation argues that large-batch methods tend to converge to sharp minimizers of the training function. Sharp minimizers are specified with lower precision and thus tend to generalize less well according to the [minimum description length (MDL) theory](https://projecteuclid.org/journals/annals-of-statistics/volume-11/issue-2/A-Universal-Prior-for-Integers-and-Estimation-by-Minimum-Description/10.1214/aos/1176346150.full).
<p align="center">
	<img src="{{ site.url }}/public/images/training_function.png" height="200" />
</p>

### Local SGD
To reduce the communication burden in mini-batch SGD, local SGD has been developed and studied extensively in recent years. It runs the SGD algorithm on each worker locally and parallelly and only averages the updates occasionally depending on the frequency of communication. The local updates [save the effort to communicate](https://papers.nips.cc/paper/2019/hash/4aadd661908b181d059a117f02fbc9ec-Abstract.html) across workers during the training process.
Local SGD for K workers:
<p align="center">
	<img src="{{ site.url }}/public/images/local_sgd.png" height="35" />
</p>
Various research papers have proved a linear speedup of local SGD with respect to the number of workers, under the assumption that the number of local steps H is not too large. Both [Zhang et al. (2016)](https://arxiv.org/abs/1606.07365) and [Bijral et al. (2016)](https://arxiv.org/abs/1603.04379) illustrate that more frequent averaging and communication at the beginning of local SGD improves rate of convergence.
The scalability and generalizability of the local SGD are tested to be more robust compared to mini-batch SGD. The reported speedups of increasing local steps are consistently different from the H = 1 (mini-batch SGD) counterpart. For H = 8, the time-to-accuracy score increases linearly with respect to the number of workers. Local SGD also achieves higher test accuracy, specially when the algorithm evaluate the same number of gradients between steps ($B = H B_{loc} $). As the number of workers and local steps increase, the gap between the test accuracy of local SGD and mini-batch SGD becomes more pronounced. On the other hand, local SGD still encounters generalization issue when scaling to larger batch size ($ H B_{loc} $), as indicated in the second plot of the first row below.
<p align="center">
	<img src="{{ site.url }}/public/images/local_sgd_generalization.png" height="200" />
</p>
<center>
 Scaling and generalization bahavior of local SGD for training ResNet-20 and CIFAR-10 with $ B_{loc} =128 $. $ H = 1 $ recovers mini-batch SGD.
</center>
## Post-local SGD: Large-batch Training Alternative

### Method Overview

Post-local SGD is one of the two noval training methods that can give better training results. Simply put, post-local SGD is a training method combining mini-batch SGD with local SGD. Mini-batch SGD is the well-known, standard SGD method for distributed training. As is illustrated in the figure below, the standard mini-batch SGD synchronizes local networks every training step, while local SGD averages out networks on different devices every several steps, depending on a configurable hyperparameter. In the illustration it is set to 3. Post-local SGD training starts off with mini-batch SGD. After training a while with standard mini-batch SGD, the training is switched to use local SGD for synchronization, all the way to the end.

<p align="center">
	<img src="{{ site.url }}/public/images/sgd_update.png" height="200" />
</p>

### Experiments & Analysis

Here is the result of applying post-local SGD training scheme on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset using [ResNet-20](https://arxiv.org/abs/1512.03385). Experiments are indexed from A1 to A5 for reference and $ K, B, H $ represent 3 configurable hyperparameters denoting the number of devices, the batch size, and the global sync interval for local SGD, respectively. Here $ B_{loc} $ is the base local batch size, which is set as 128 across all experiments. The experiment A5, which is the proposed post-local SGD, performs the best among all, in terms of test top-1 accuracy. This indicates the effectiveness of the post-local SGD scheme.

<p align="center">
	<img src="{{ site.url }}/public/images/sgd_exp.png" height="300" />
</p>

#### Advantage #1: Improving Scalability

One obvious benefit that this proposed method brings to us is that it can reduce the communication between workers during the training since now it only periodically synchronizes across the workers instead of keeping them sync-ed every training step. Hence, it effectively reduces network usage and speeds up the training. As is shown in the figure below, when doing the training using 16 workers, we can get 30x speed-up if we use post-local SGD and set the synchronization interval to be 8, which drastically reduces the total training time.

<p align="center">
	<img src="{{ site.url }}/public/images/sgd_scale.png" height="150" />
</p>

#### Advantage #2: Improving Generalizability

Another interesting discovery is that comparing experiment A2, which utilizes mini-batch SGD, and experiment A5, which uses post-local SGD. They both have the same batch size, same level of scalability (i.e. both are trained on 16 devices). We can clearly see the gap between the training top-1 accuracy and the testing top-1 accuracy is different. Specifically, A5 has a lower training top-1 accuracy (99% vs. 100% when compared with A2) but higher testing top-1 accuracy (93% vs. 92% when compared with A2). This shows that post-local SGD does not overfit the training set and improves the generalizability of the model.

### Afterthoughts about Post-local SGD

After reading through the paper, some legit concerns do come into my mind.

<!--#### Weakness #1: Poor Performance Using Local SGD Only

Looking back to the experiment A4, which only uses the local SGD throughout the training, its performance can only be described as being mediocre. The state-of-the-art performance can only be achieved whithout using mini-batch SGD to warm up the training a little bit, hence formulating the procedure of post-local SGD. Due to this, it casts some doubt into the effectiveness of the local SGD method since it seems that using it alone is not sufficient. The authors did not provide any insight into this issue.-->

#### Weakness #1: Ad-hoc Hyperparameter Choices

<p align="center">
	<img src="{{ site.url }}/public/images/hyperparam.png" height="200" />
</p>

Other than doing random searching, the authors did not explain how they get their hyperparameters w.r.t. when to switch from mini-batch SGD to local SGD as well as the size of the synchronization interval. For a method that does not have good theorectical support, the intuition behind the final hyperparameters should be well-explained. This is especially true here as we clearly see that the performance does not always increase when increasing the size of the synchronization interval or the number of mini-batch SGD warm-up epochs (i.e. the optimal values are not easily seen through). Hence, the hyperparmeter choices made by authors seem to be ad-hoc.

#### Weakness #2: Little Improvement on Other Datasets

In supplementary material, the authors reported the experiment result of post-local SGD on [ImageNet](https://www.image-net.org/about.php) using [ResNet
-50](https://arxiv.org/abs/1512.03385), which at best gives 0.31% gain in terms of the testing top-1 accuracy. This sounds pretty incremental and the size of the synchronization interval is pretty small (i.e. only to be 4), which does not reflect that the local SGD is of great help to the training.

Also on language modeling task on [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), the post-local SGD result is worse than the large batch baseline, which weakens the claim that local SGD helps the training significantly. Further discussion should be held to address these issues.


## Hierarchical Local SGD: Optimal Use of Systems Resources

### Method Overview

Hierarchical local SGD focuses on the optimal usage of systems resources in heterogeneous systems. It takes into account the hierarchical levels of communication bandwidths within a system and tries to optimize the computation vs communication trade-off on each level by running the local SGD on each level. Besides local updates on each device, it includes another loop, which averages and updates the gradients on each node. As illustrated in the following picture, the resulting algorithm runs local SGD on each node of the system and uses both block and global synchronization to communicate the updates. 

<p align="center">
	<img src="{{ site.url }}/public/images/HL_SGD.png" height="450" />
</p>

This design can be particularly useful when the communication bandwidths are different on each level. Imagine that we have multiple GPU-blocks, and the communication speed between GPUs in the same block can be faster than the communication speed between GPUs from different blocks. Hierarchical SGD computes gradient updates on each GPU locally for H times and then synchronizes the updates across the GPUs for each GPU-blocks. Finally, after $ H^{b} $ block updates, the synchronization is performed globally across the GPU-blocks. Reducing H or increasing $ H^{b} $ decreases the relative frequency of global synchronization, thus improving the model training time.

### Experiments & Analysis

#### Advantage #1: Reducing Communication Cost

Running on GPU clusters, local SGD has limited ability to reduce training times when increasing the number of local updates. However, hierarchical local SGD can reduce the communication burden by adding an intermediate step of block synchronization. The within-block communication mitigates the difficulty of synchronizing across heterogeneous levels. By increasing the number of block updates $H^{b}$, hierarchical local SGD achieves significantly less training time without serious reduction of training accuracy. The algorithm is also robust to network latency. The difference in training time is even more pronounced when there is a longer delay.

<p align="center">
	<img src="{{ site.url }}/public/images/HL_SGD_Training_Time.png" height="50" />
</p>

<p align="center">
Training time of local SGD trained on CIFAR-10 with ResNet-20 
on 8 nodes with 2 GPUs/node
</p>

<p align="center">
	<img src="{{ site.url }}/public/images/HL_SGD_Training_Accuracy.png" height="200" />
</p>

<p align="center">
Performance of hierarchical local SGD trained on CIFAR-10 with ResNet-20
using 2 nodes with 2 GPUs/node with 0, 1s, and 50s delay for each global synchronization.
</p>

#### Advantage #2: Improving Generalizability Even Further

Hierarchical local SGD can also improve the generalization performance of local SGD ($ H^{b}=1 $). When $ HH^{b} $ is fixed, hierarchical local SGD outperforms local SGD if block synchronization is frequent and the network has a sufficiently large block size. The experiment also reveals that the generalization ability of hierarchical local SGD depends on the network topologies. Further investigation of the behavior of hierarchical local SGD on larger networks will help us understand its adaptivity to different topologies and its scalability when increasing the size of the network.

<p align="center">
	<img src="{{ site.url }}/public/images/HL_SGD_Performance_Table.png" height="200" />
</p>

<p align="center">
Testing performance of training CIFAR-10 with ResNet-20 via hierarchical local SGD on a 16-GPU Kubernetes cluster with different network topologies. 
</p>



## Conclusions and Final Thoughts
The above experiments have suggested that local SGD algorithm and its variants, post-local SGD and hierarchical SGD, result in significant improvement over mini-batch SGD, especially for large batch sizes. The setup of the experiments, however, adopts a learning scheme designed and tuned for mini-batch SGD, which ignores the effect of local updates in the tested algorithms. To better understand the efficacy of local SGD and its variants, we should develop specific learning schemes better suited to the algorithms.

There is also potential for further improvements of the algorithms. The number of local updates and block synchronization are extra parameters that should be studied more carefully to understand their impact on optimization performance. They can also be chosen adaptively in the training procedures.
